{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e479275b",
   "metadata": {},
   "source": [
    "## GOAL: predict whether an edge (interaction) between two proteins should be included in a pathway (label 1) or not (label 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c50aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import EdgeConv, NNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae6dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7030fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f94abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device to CUDA\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45e575",
   "metadata": {},
   "source": [
    "### Data Loading:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7975f71",
   "metadata": {},
   "source": [
    "**Pytorch Geometric `DataLoader` object from saved data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f8a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique nodes: 17407\n"
     ]
    }
   ],
   "source": [
    "union_ppi = pd.read_csv('processed-data/union_ppi.txt', sep='\\t', header=None)\n",
    "unique_nodes = set(union_ppi[0].tolist() + union_ppi[1].tolist())\n",
    "label_id_map = {label: idx for idx, label in enumerate(sorted(unique_nodes))}\n",
    "num_nodes = len(label_id_map)\n",
    "print(f\"Total unique nodes: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60700cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = torch.load('dataset.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c54c2f",
   "metadata": {},
   "source": [
    "### Class Weight Calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248abcef",
   "metadata": {},
   "source": [
    "- Calculate the weight for the positive class to handle class imbalance\n",
    "- needed since the number of negative samples (non-selected edges) is much higher than positive samples (selected edges)\n",
    "- weighting the **loss function** helps the model to learn from the minority class effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8bddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive class weight: 14725.188571936373\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(data_list):\n",
    "    # concat all labels into single tensor\n",
    "    y = torch.cat([data.y for data in data_list]).cpu().numpy()\n",
    "    # count the number of pos and negative samples\n",
    "    num_positive = y.sum()\n",
    "    num_negative = len(y) - num_positive\n",
    "    # calculate the weight --> ratio of negative samples to positive samples\n",
    "    # so that loss function balances the contribution of both classes\n",
    "    pos_weight = torch.tensor([num_negative / num_positive]).to(device)\n",
    "    return pos_weight\n",
    "\n",
    "pos_weight = calculate_class_weights(data_list)\n",
    "print(f\"Positive class weight: {pos_weight.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404afc8",
   "metadata": {},
   "source": [
    "### Train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e89fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(data_list))\n",
    "test_size = len(data_list) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c900326",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(data_list, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab2737",
   "metadata": {},
   "source": [
    "### Edge Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b063fd6",
   "metadata": {},
   "source": [
    "- technique that oversamples the positive edges to address class imbalance\n",
    "- duplicate positive edges in dataset to increase representation during training - so that the model sees more examples\n",
    "- oersampling helps the model to better learn the characteristics of the minority class (selected edges), improving its ability to classify them correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9c7763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_sampling(data_list, sampling_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Oversample edges from the minority class (positive edges).\n",
    "    Args:\n",
    "        data_list: List of PyG data objects.\n",
    "        sampling_ratio: Ratio of minority class edges to add.\n",
    "    Returns:\n",
    "        Augmented data_list with oversampled positive edges.\n",
    "    \"\"\"\n",
    "    augmented_data_list = []\n",
    "    for data in data_list:\n",
    "        y = data.y.cpu().numpy()\n",
    "        # get the indices of positive and negtive edges\n",
    "        positive_indices = np.where(y == 1)[0]\n",
    "        negative_indices = np.where(y == 0)[0]\n",
    "        # oversample positive edges\n",
    "        num_positive = len(positive_indices)\n",
    "        # fraction of +ve edges are randomly sampled with replacement\n",
    "        num_samples = int(sampling_ratio * num_positive)\n",
    "        # stores the indices of the sampled positive edges\n",
    "        sampled_indices = np.random.choice(positive_indices, num_samples, replace=True)\n",
    "        # connectivity information (source and target nodes) for the sampled edges:\n",
    "        sampled_edge_index = data.edge_index[:, sampled_indices]\n",
    "        # edge features for sampled edges:\n",
    "        sampled_edge_attr = data.edge_attr[sampled_indices]\n",
    "        # labels for sampled edges:\n",
    "        sampled_y = data.y[sampled_indices]\n",
    "        # connectivity information is updated by concatenating the original edges and the sampled edge:\n",
    "        data.edge_index = torch.cat([data.edge_index, sampled_edge_index], dim=1)\n",
    "        # edge features are updated by concatenating the original features and the sampled features:\n",
    "        data.edge_attr = torch.cat([data.edge_attr, sampled_edge_attr], dim=0)\n",
    "        # labels are updated by concatenating the original labels and the sampled label:\n",
    "        data.y = torch.cat([data.y, sampled_y], dim=0)\n",
    "        augmented_data_list.append(data)  \n",
    "    return augmented_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3c682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply edge sampling to the training dataset\n",
    "train_dataset = edge_sampling(train_dataset, sampling_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda39fc",
   "metadata": {},
   "source": [
    "### Make DataLoader:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d7242",
   "metadata": {},
   "source": [
    "- for batching and shuffling data\n",
    "- feeds data into model during training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a449089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe1879",
   "metadata": {},
   "source": [
    "### Model Definition: using `EdgeConv` and `NNNConv` layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc48df",
   "metadata": {},
   "source": [
    "- define GNN for edge classification \n",
    "- architecture designed to capture both node + edge features - needed for accurately classifying edges in PPI network\n",
    "- use mlps to transform node + edge features into higher dim representations to capture pattersn in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa75d1c",
   "metadata": {},
   "source": [
    "**Input Graph:**\n",
    "- Node features (x): `[num_nodes, node_feat_dim]`\n",
    "- Edge index (edge_index): `[2, num_edges]`\n",
    "- Edge features (edge_attr): `[num_edges, edge_feat_dim]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613857d8",
   "metadata": {},
   "source": [
    "**Layers:**\n",
    "- `EdgeConv` captures local patterns in the graph by considering the relationships between a node and its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e3c9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeClassificationGNN(nn.Module):\n",
    "    def __init__(self, node_feat_dim, hidden_dim, edge_feat_dim, out_dim=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_feat_dim (int): Dimensionality of node features (1 in this case).\n",
    "            hidden_dim (int): Hidden layer dimension.\n",
    "            edge_feat_dim (int): Dimensionality of edge features (1 for prize).\n",
    "            out_dim (int): Output dimension (1 for binary classification).\n",
    "        \"\"\"\n",
    "        super(EdgeClassificationGNN, self).__init__()\n",
    "        \n",
    "        # --- First Layer: EdgeConv ---\n",
    "        # PURPOSE: updates the features of each node by aggregating information from its direct neighbors.\n",
    "        # MLP takes the concatenated features of a node and its neighbor --> outputs new feature vector\n",
    "        self.mlp_edgeconv = nn.Sequential(\n",
    "            nn.Linear(2 * node_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        # resutls from all neighbors are aggregated - \n",
    "        # 'max' selects the most important feature for each dimension\n",
    "        self.conv1 = EdgeConv(nn=self.mlp_edgeconv, aggr='max')\n",
    "        \n",
    "        # --- Second Layer: NNConv ---\n",
    "        # PURPOSE: updates node features by incorporating edge-specific information\n",
    "        # MLP maps edge features to weight matrix --> which transforms the neighboring node features\n",
    "        self.edge_nn = nn.Sequential(\n",
    "            nn.Linear(edge_feat_dim, hidden_dim * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * hidden_dim, hidden_dim * hidden_dim)\n",
    "        )\n",
    "        # transformed features from all neighbors aggregated by averaging contriutions from all neihbors\n",
    "        self.conv2 = NNConv(in_channels=hidden_dim,\n",
    "                            out_channels=hidden_dim,\n",
    "                            nn=self.edge_nn,\n",
    "                            aggr='mean')\n",
    "        \n",
    "        # --- Final Edge Classifier ---\n",
    "        # combine the features of the source node, target node, and edge to predict the edge label\n",
    "        # classifier is mlp that takes concatenated features and outputs raw score i.e. logit for binary classification\n",
    "        self.edge_classifier = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + edge_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        # input node features and edge index into edge conv\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # input updated node features, edge index (edge_index), and edge features (edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        # for edge classification\n",
    "        src, dst = edge_index\n",
    "        edge_representation = torch.cat([x[src], x[dst], edge_attr], dim=1)\n",
    "        logits = self.edge_classifier(edge_representation)\n",
    "        # return raw logits instead of probabilities\n",
    "        return logits  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2630d50",
   "metadata": {},
   "source": [
    "### Training Setup & Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8f37d",
   "metadata": {},
   "source": [
    "- initialize model, loss function, and optimizer\n",
    "- `BCEWithLogitsLoss` loss function is chosen for binary classification - binary CEL with logitcs i.e raw scores\n",
    "- `Adam` optimizer users to update model parameters during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1c2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 8 # dimension of hidden layer\n",
    "node_feat_dim = 1 # dimension of node features\n",
    "edge_feat_dim = 1 # dimension of edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e38a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EdgeClassificationGNN(node_feat_dim, hidden_dim, edge_feat_dim)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80be6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use class weighting in the loss function - assign higher weight to positive class i.e. minority class\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1d34b",
   "metadata": {},
   "source": [
    "- train the model using the following loop function \n",
    "- update model params to minimize loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "159545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs=20):\n",
    "    model.train()  # set the model to training mode bc of layers beaviours\n",
    "    # epoch loop:\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)  # move batch to the GPU\n",
    "            # clear grads from previous batch\n",
    "            optimizer.zero_grad()  \n",
    "            # forward pass + get model predictions based on input batch\n",
    "            out = model(batch) \n",
    "            # compute loss by comparing model pred (out) to ground truth labels\n",
    "            loss = criterion(out, batch.y)  \n",
    "            # compute grads wrt model params - backprop\n",
    "            loss.backward()  \n",
    "            # update model params using computed gradients - optimization\n",
    "            optimizer.step() \n",
    "            # accumulate loss for current epoch\n",
    "            epoch_loss += loss.item()  # Accumulate loss    \n",
    "        # print average loss for the epoch\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b907b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjeevs/.local/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3782\n",
      "Epoch 2/10, Loss: 0.2979\n",
      "Epoch 3/10, Loss: 0.0443\n",
      "Epoch 4/10, Loss: 0.0154\n",
      "Epoch 5/10, Loss: 0.0047\n",
      "Epoch 6/10, Loss: 0.0024\n",
      "Epoch 7/10, Loss: 0.0016\n",
      "Epoch 8/10, Loss: 0.0011\n",
      "Epoch 9/10, Loss: 0.0010\n",
      "Epoch 10/10, Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "train(model, train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961db6b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34aec8",
   "metadata": {},
   "source": [
    "- Evaluates the model's performance using precision, recall, F1-score, and AUC-ROC.\n",
    "- These metrics provide a comprehensive assessment of the model's classification performance, especially important for imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "890d83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            preds = (out > 0).float()  # Use 0 as threshold for logits\n",
    "            y_true.extend(batch.y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_score.extend(out.sigmoid().cpu().numpy())  # Probabilities for AUC-ROC\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC-ROC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc8dc10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5339, Recall: 1.0000, F1: 0.6962, AUC-ROC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "evaluate_metrics(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b150b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
